{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This notebook is created based on the ideas, discussion and effort of Balaji, Mimi and Arjun"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport os\nimport random\nimport warnings\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set random seeds (for reproducibility requirement)\nos.environ['PYTHONHASHSEED']=str(1)\nnp.random.seed(1)\nrandom.seed(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Loading the Datasets"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load data\ntrain = pd.read_csv('../input/ift6758-a20/train.csv')\ntest = pd.read_csv('../input/ift6758-a20/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tid=test['Id']\ntest_id=tid.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x=train.iloc[:,:24]\ntrain_y=train.iloc[:,23]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loc=train_x.copy()\ntrain_loc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loc.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Columns**\n\nId - Useful for submission, Removed during training\n\nUser Name - Removed\n\nPersonal URL - 0 for Nan,1 otherwise\n\nUser Name - The screen name of the user\n\nProfile Cover Image Status - 0/1\n\nProfile Verification Status - 0/1\n\nProfile Text Color - Removed\n\nProfile Page Color - Removed\n\nProfile Theme Color - Removed\n\nIs Profile View Size Customized? - 0/1\n\nUTC Offset - Removed\n\nLocation - Removed\n\nLocation Public Visibility - 0/1\n\nUser Language - converted to few unique values\n\nProfile Creation Timestamp - converted to months from date\n\nUser Time Zone - Converted to 7 values\n\nNum of Followers - log transform\n\nNum of People Following - log transform\n\nNum of Status Updates - log transform\n\nNum of Direct Messages - log transform\n\nProfile Category - label encoding\n\nAvg Daily Profile Visit Duration in seconds - log transform\n\nAvg Daily Profile Clicks - log transform\n\nProfile Image - Removed\n\nNum of Profile Likes - Label"},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Preprocessing and Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing 8 columns that are not being used\ndef drop_columns(df):\n    df.drop(['Id','User Name','Location','UTC Offset','Profile Image','Profile Text Color',\n               'Profile Page Color','Profile Theme Color'],axis=1,inplace=True)\n\n# 15 feature columns are left after these columns are removed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_columns(train_loc)\ndrop_columns(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_zone_dict = {\n'Eastern Time (US & Canada)':'USA',\n'Pacific Time (US & Canada)':'USA',\n'Central Time (US & Canada)':'USA',\n'Central Time (US & Canada)':'USA',\n'London':'Europe',\n'Brasilia':'Latin America',\n'Paris':'Europe',\n'Quito':'Latin America',\n'Jakarta':'Asia',\n'Amsterdam':'Europe',\n'Mexico City':'Europe',\n'Madrid':'Europe',\n'New Delhi':'Asia',\n'Istanbul':'Middle East',\n'Hawaii':'USA',\n'Tokyo':'Asia',\n'Rome':'Europe',\n'Santiago':'Latin America',\n'Greenland':'Europe',\n'Buenos Aires':'Europe',\n'Mountain Time (US & Canada)':'USA',\n'Riyadh':'Middle East',\n'Caracas':'Latin America',\n'Athens':'Europe',\n'Atlantic Time (Canada)':'USA',\n'Bern':'Europe',\n'Alaska':'USA',\n'Arizona':'USA',\n'Bogota':'Latin America',\n'Mumbai':'Asia',\n'India':'Asia',\n'Berlin':'Europe',\n'Hong Kong':'Asia',\n'Seoul':'Asia',\n'Pretoria':'Africa',\n'Sydney':'Asia',\n'Muscat':'Middle East',\n'Baghdad':'Middle East',\n'Dublin':'Europe',\n'Berlin':'Europe',\n'Casablanca':'Africa',\n'Cairo':'Africa',\n'Abu Dhabi':'Middle East',\n'Chennai':'Asia',\n'Kuwait':'Middle East',\n'Kuala Lumpur':'Asia',\n'Brussels':'Europe',\n'Moscow':'Asia',\n'Central America':'Latin America',\n'Ljubljana':'Europe',\n'Singapore':'Asia',\n'Melbourne':'Asia'}\n\ndef location_fix(df):\n    '''\n    this function is to replace city with continent\n    '''\n    for i in time_zone_dict.items():\n        df['User Time Zone'] = df['User Time Zone'].replace(i[0], i[1])\n\n    top_used_loc=['USA','Europe','Latin America','Asia','Middle East','Africa']\n    df['User Time Zone'][~df['User Time Zone'].isin(top_used_loc)]='Others'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"location_fix(train_loc)\nlocation_fix(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing_num(df):\n    # Converting personal url to binary\n    df['Personal URL'].fillna(0,inplace=True)\n    df['Personal URL'][df['Personal URL']!=0]=1\n\n    # Converting '??' from the Location Public Visibility to enabled\n    df['Location Public Visibility']=df['Location Public Visibility'].str.lower()\n    df['Location Public Visibility']=df['Location Public Visibility'].replace('??','enabled')\n    \n    # These four languages are the most common. Other languages are converted to 'others'\n    top_used_lang=['en','es','pt','fr']\n    df['User Language'][~df['User Language'].isin(top_used_lang)]='others'\n\n    # ' ' value in Profile Category  column is converted to 'unkown'\n    df['Profile Category']=df['Profile Category'].replace(' ','unknown')\n    \n    # Here we do a log transform for four continuous valued inputs to remove the skew in the features and \n    # get feature values that resembles a normal distribution.\n\n    df['Num of Followers']= np.log10(1+df['Num of Followers'])\n    df['Num of People Following']= np.log10(1+df['Num of People Following'])\n    df['Num of Status Updates']= np.log10(1+df['Num of Status Updates'])\n    df['Num of Direct Messages']= np.log10(1+df['Num of Direct Messages'])\n\n    \n    # We do a log transform of the 'Avg Daily Profile Visit Duration in seconds' column and also impute the \n    # NaN values by the mean value of the column.\n    df['Avg Daily Profile Visit Duration in seconds']=np.log10(1+df['Avg Daily Profile Visit Duration in seconds'])\n    df['Avg Daily Profile Visit Duration in seconds'].fillna((df['Avg Daily Profile Visit Duration in seconds'].mean()), inplace=True)\n\n    # Same procedure is done for 'Avg Daily Profile Clicks' column also\n    df['Avg Daily Profile Clicks']= np.log10(1+df['Avg Daily Profile Clicks'])\n    df['Avg Daily Profile Clicks'].fillna((df['Avg Daily Profile Clicks'].mean()), inplace=True)\n\n    # We fill the NaN values in 'Profile Cover Image Status' column by 'Not set'\n    df['Profile Cover Image Status'].fillna('Not set',inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessing_num(train_loc)\npreprocessing_num(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing_category(df):\n\n\n    # Now we convert the categorical column values from text form to numerical form to input it to the model\n    cleanup_nums = {\"Personal URL\": {\"0\":0, \"1\":1},\n                \"Profile Cover Image Status\":     {\"Not set\": 0, \"Set\": 1},\n                \"Profile Verification Status\": {\"Not verified\": 0, \"Pending\": 1, \"Verified\": 2 },\n                \"Is Profile View Size Customized?\":{\"False\":0,\"True\":1},\n                \"Location Public Visibility\":{'disabled':0,'enabled':1},\n                \"Profile Category\":{'unknown':0,'government':1,\"business\":2,'celebrity':3},\n                \"User Time Zone\":{'Others':0,'Africa':1,'Middle East':2,'Asia':3,'Latin America':4,'Europe':5,'USA':6},\n                'User Language':{'others':0,'fr':2,'pt':3,'es':4,'en':5}\n               }\n\n    # Converting the data type of the categorical columns to 'str'\n    df['Profile Cover Image Status'] = df['Profile Cover Image Status'].astype(str)\n    df['Profile Verification Status'] = df['Profile Verification Status'].astype(str)\n    df['Is Profile View Size Customized?'] =df['Is Profile View Size Customized?'].astype(str)\n    df['Location Public Visibility'] = df['Location Public Visibility'].astype(str)\n\n    df = df.replace(cleanup_nums)\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loc=preprocessing_category(train_loc)\ntest=preprocessing_category(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add extra columns from existing features\n\n- We extract month in social media from `Profile Creation Timestamp` and store it as `MonthsInSocialMedia`.\n- A new column, `Months follower ratio` is from `Num of Followers` divided by `MonthsInSocialMedia`.\n- A new column, `Months following ratio` is from `Num of People Following` divided by `MonthsInSocialMedia`.\n- A new column, `Months status ratio` is from `Num of Status Updates` divided by `MonthsInSocialMedia`.\n- A new column, `Months messages ratio` is from `Num of Direct Messages` divided by `MonthsInSocialMedia`.\n- A new column, `group_sum` is from taking summation of other numerical features, and then divided by 6 to adjust the scale.\n- A new column, `Total Activity` is from adding `Num of Status Updates` and `Num of Direct Messages` together.\n- A new column `Total clicks from inception` is from multiply number of days in a month (30) to `Avg Daily Profile Clicks` and `MonthsInSocialMedia`."},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_columns(df):\n    # Convert the time stamp column into a new column that represents the number of months \n    # the person has been on social media\n    df['Profile Creation Timestamp'] = df['Profile Creation Timestamp'].astype(str)\n    df['Profile Creation Timestamp'] =pd.to_datetime(df['Profile Creation Timestamp'])\n    df['MonthsInSocialMedia'] = ((2020- df['Profile Creation Timestamp'].dt.year) * 12 +\n    (11 - df['Profile Creation Timestamp'].dt.month))\n        ### new \n    df['MonthsInSocialMedia'] =np.log10(1+df['MonthsInSocialMedia'])    \n        \n    df['Months follower ratio']=df['Num of Followers']/df['MonthsInSocialMedia']\n    df['Months following ratio']=df['Num of People Following']/df['MonthsInSocialMedia']\n    df['Months status ratio']=df['Num of Status Updates']/df['MonthsInSocialMedia']\n    df['Months messages ratio']=df['Num of Direct Messages']/df['MonthsInSocialMedia']\n    group_col = df[['Num of Followers', 'Num of People Following', 'Num of Status Updates', 'Num of Direct Messages','Avg Daily Profile Visit Duration in seconds', 'Avg Daily Profile Clicks']]\n    df['group_sum'] = np.sum(group_col, axis=1)\n    df['group_sum']=df['group_sum']/6\n    \n    df['Total Activity']=df['Num of Status Updates']+df['Num of Direct Messages']\n    df['Total clicks from inception']=df['Avg Daily Profile Clicks']*30*train_loc['MonthsInSocialMedia']\n#     df['Num of People Following'][df['Num of People Following']<=0]=0.1\n#     df['Followers Ratio']=df['Num of Followers']/df['Num of People Following']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_columns(train_loc)\nnew_columns(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We drop the 'Profile Creation Timestamp' because we extracted the useful information from this column and stored it in 'MonthsInSocialMedia' column."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_loc.drop('Profile Creation Timestamp',axis=1,inplace=True)\ntest.drop('Profile Creation Timestamp',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop Label"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loc.drop(['Num of Profile Likes'],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training data has been stored in a different variable for convinience.\ntrain_ax =train_loc.copy()\ntrain_ay = train_y.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_x_all = train_ax\nfit_y_all = np.log10(1+train_ay)\npred = test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr = SVR(kernel='rbf', epsilon=0.2,C=0.75)\n\nxgboost = XGBRegressor(learning_rate=0.03,\n                       n_estimators=250,\n                       max_depth=3,\n                       seed=27,\n                       alpha=2,\n                       random_state=1)\n\nxgboost2 = XGBRegressor(learning_rate=0.03,\n                       n_estimators=200,\n                       max_depth=4,\n                       seed=400,\n                       alpha=0,\n                       random_state=34)\n\n\nsvr2 = SVR(kernel='rbf', epsilon=0.1,C=0.75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stack = StackingCVRegressor(regressors=(xgboost, xgboost2,svr2, svr),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True,random_state=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stack_te = make_pipeline(StandardScaler(), stack).fit(fit_x_all, fit_y_all)\ntest_predl = stack_te.predict(pred)\ntest_pred =(10**test_predl) - 1\ntest_pred[test_pred < 0] = 0\noutput = np.round_(test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub =  open('final_stack_xgboost_svr.csv','w+')\nsub.write('Id,Predicted\\n')\nfor index, prediction in zip(test_id,output):\n    sub.write(str(index) + ',' + str(prediction) + '\\n')\nsub.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}